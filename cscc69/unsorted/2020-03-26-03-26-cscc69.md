

* Parallel request
* Geo location (replicas in different locations)
* Single point of failure
* Data size limitation
* Data hotspot
* Service hotspot

App server behind load balancer
* Distribute the load with a load balancer, and have some round robin algorithm
	to cycle between them.
* Parallel request
* Service hotspot
* Single point of failure
________________
|               |
| load balancer |
|_______________|

  |     |    |
 []     []   []

 Data replication:
 * Replicate data in different timezones, and continents
 * Geo location - If data is lost in a local data center, it will be on another
	 server somewhere else
 * Single point of failure - Data isn't stored in one place
 * Parallel requests

[est]
[west]
[europe]

Sharding:
* Partitioning your data across many databases. Can be in the level of
	databases, filesystems, cache.
* Sharding is always row based. Store first 100mil tweets of American tweets in
	data centers in Europe
* Data size limitation
* Parallel requests to different locations, reduces data hotspotting
* Requirements:
	* Even distribution of data and shards
	* Must be easy to add and remove new shards
	* shards must be available
* Simple design:
	* N shards
	* Hash function % N
	* Problem: adding new shards expensive
		* Changing from N to N+1, have to redistribute
	* Problem: what if 1 shard is not available
* Consistent hashing:
	* Map m shards to n data points
	* Forms a sort of circular mapping
	* All users mapped between 0 and S1 will be served by S1
	* Anything after Sm will be served by S1
	* Always replicate the data of S1 in S2
![sharding](./pictures/sharding.png)
![consistent-hashing](./pictures/consistent-hashing.png)
___________________________________________
| tweet_id | text | creation time | likes |
|
|

Caching:
* Stores subset of data in the storage
* Instead of sending all read requests to storage, read requests can go to
	cache, and reduces data hot spotting

Microservices:
* Instead of one big service, build your app as a collection of loosely-couple
	microservices. Each one exposes an API
	* One service for tweeting
	* One service for signing up, etc.
* Better for scaling up independently of other microservices (serveral
	instances of tweeting microservices)
* Loosely coupled. Don't depend on each other, but can still communicate

![microservices](./pictures/microservices.png)

Storage (Logical Schema):
* Database tables
* Relational model (primary key that uniquely identifies a tweet in the system) 
![relational-model](./pictures/relational-model.png)
* Key-Value Pair
	* Values can be KVP, Json, XML, Blobs (links to files,
	pictures).
	* Like a hash table, so accessing data is potentially faster
	* NoSQL
![key-value-pair](./pictures/key-value-pairs.png)
* Graph Model
	* Stores nodes and the relationship between nodes
	* Used in social networking problems
![graph-model](./pictures/graph-model.png)

Storage (Physical Schema):
* How data is stored on the hard drive
* Row Major
	* More common in major databases
	* All columns in rows
![row-major](./pictures/row-major.png)
* Column Major
	* Newer than row major
	* Column familes
	* Multiple arrays
	* Replicate the primary key, to connect them together
![column-major](./pictures/column-major.png)

Replication:
* Replicate data in many places
* Have to propagate changes in one place to all the other places
* Different views of the database in different places
* Assign timestamps, must have exact same time across time zones (UTC makes
	sure that happens)
* You can start replication right after step 2
![replication](./pictures/replication.png)

![replicating-steps](./pictures/replication-steps.png)
