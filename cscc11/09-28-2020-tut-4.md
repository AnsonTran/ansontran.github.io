$p(w|d) = \frac{P(D|w)P(w)}{P(D)}

Posterior: $P(w|d)$
Likelihood: $P(D|w)$
Prior: $P(w) = \integral p(d|w)p(w)dx$

P(M|D) = p(D|M)p(M)/P(D)

1. Maximum Likelihood Estimator
p(y|w*x) = \mult_{i=1}^{*}P(y_i|w,x_i) (independent)
(likelihood)

\hat{w} = argmax P(y|w, x)

2. MAP (Maximum Posterior Estimation)
e.g. w ~ N(.,.)
\hat{w} = argmax P(y|w,x)p(w)
\hat{w} = argmax P(w|y,z)

argmax on the posterior distribution

MAP = MLE
if assume w follows a uniform distribution
no matter each w we plug in this expression, p(w) gives the same value
so it has no effect on the MAP decision, so equal to MLE case

3. Baye's Estimator
p(y|x,w) = \integral p(y|x, w) p(w)dw

predictive distribution


Example:

x_i ~ N(\mew_i, \sigma)

P(x|\mew, \sigma) = \Mult _{i-1}^{n} p(x_i|\mew, \sigma)
Assume they are indepent, and independently distributed


x_i \in R^d, \forall i
= \mult_{i=1}^{n} \frac{1}{\sqrt{(2x)*|\sigma|}}exp[-1/2(x_i-\mew)^T\sigma^{-1}(x_i-\mew)]
Expand into the multivariate gaussian

L = -log_eP(x|\mew, \sum) = -\sum_{i=1}^{n}-1/2(x_i-\mew)^T\sum^{-1}(x_i-\mew)-n/2log\vert\sum\vert - nd/2*log(2x)
= \sum_{i=1}^{n} \frac{(x_i-\mew)^T\sum^{-1}(x_i-\mew)}{2} + \sum^{n} log
\vert\sum\vert + \frac{nd}{2}log(2\pi)
Take the log probability

Next step: \hat{\mew} = argmax P(x|\mew, \sum) = argmin L
\hat{\sum} = argmax P(x|\mew, \sigma) = argmin L

pdv{L}{\mew} = \pdv{}{\mew} [\sum_{i=1}^{n} \frac{(x_i -
\mew)^T\sum^{-1}(x_i - \mew)}{2} + \sum^{n} log \vert\sum\vert +
\frac{nd}{2} log(2x_i)]
= \sum_{i=1}^{n} \pdv{}{\mew} [\frac{(x_i-\mew)^T\sum^{-1}(x_i - \mew)}{2}
\sum_{i=1}^n - \sum^{-1}(x_i - \hat{\mew}) = 0
\sum_{i=1}^{n}(x_i - \hat{\mew}) = 0
\hat{mew} = \frac{1}{n} \sum_{i=1}^n x_i

\hat{\mew} = \frac{1}{n}\sum_{i=1}^{n}x_i % sample mean
\hat{\sum} = \frac{1}{n}\sum_{i=1}^{n}(x_i - \hat{\mew})(x_i-\hat{mew})^T % sample variance

Entropy: H:= -\sum_{c=1}^{k}p_c\log p_c

Uniform distributions, more uncertainty, H large
Gaussian distributions, less uncertainty, H small

Suppose you have 8 posibilities, 1/8 probability each
We don't know which one will occur, out of the 8 possibilities

H = - \sum_{c=1}^{8} \frac{1}{3}(-3) = 3

Suppose you have two scenarios, A=0 and B=0
More certainty

H = - \sum_{c=1}^{1} 0 * something - \vert log \vert = 0
